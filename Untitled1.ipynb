{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cherokee, Oklahoma</td>\n",
       "      <td>0</td>\n",
       "      <td>It is the county seat of Alfalfa County .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cherokee, Oklahoma</td>\n",
       "      <td>0</td>\n",
       "      <td>Cherokee is a city in Alfalfa County , Oklahom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Skateboard</td>\n",
       "      <td>5</td>\n",
       "      <td>Skateboard decks are usually between 28 and 33...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Skateboard</td>\n",
       "      <td>5</td>\n",
       "      <td>The underside of the deck can be printed with ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Skateboard</td>\n",
       "      <td>6</td>\n",
       "      <td>This was created by two surfers ; Ben Whatson ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    0  1                                                  2\n",
       "0  Cherokee, Oklahoma  0          It is the county seat of Alfalfa County .\n",
       "1  Cherokee, Oklahoma  0  Cherokee is a city in Alfalfa County , Oklahom...\n",
       "2          Skateboard  5  Skateboard decks are usually between 28 and 33...\n",
       "3          Skateboard  5  The underside of the deck can be printed with ...\n",
       "4          Skateboard  6  This was created by two surfers ; Ben Whatson ..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal = pd.read_csv('./data/sentence-aligned.v2/normal.aligned',sep='\\t',header=None)\n",
    "normal.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cherokee, Oklahoma</td>\n",
       "      <td>0</td>\n",
       "      <td>It is the county seat of Alfalfa County .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cherokee, Oklahoma</td>\n",
       "      <td>0</td>\n",
       "      <td>Cherokee is a city of Oklahoma in the United S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Skateboard</td>\n",
       "      <td>2</td>\n",
       "      <td>Skateboard decks are normally between 28 and 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Skateboard</td>\n",
       "      <td>2</td>\n",
       "      <td>The bottom of the deck can be printed with a d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Skateboard</td>\n",
       "      <td>3</td>\n",
       "      <td>The longboard was made by two surfers ; Ben Wh...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    0  1                                                  2\n",
       "0  Cherokee, Oklahoma  0          It is the county seat of Alfalfa County .\n",
       "1  Cherokee, Oklahoma  0  Cherokee is a city of Oklahoma in the United S...\n",
       "2          Skateboard  2  Skateboard decks are normally between 28 and 3...\n",
       "3          Skateboard  2  The bottom of the deck can be printed with a d...\n",
       "4          Skateboard  3  The longboard was made by two surfers ; Ben Wh..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple = pd.read_csv('./data/sentence-aligned.v2/simple.aligned',sep='\\t',header=None)\n",
    "simple.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_samples = 100_000\n",
    "num_val_samples = 10_000\n",
    "embedding_dim = 100\n",
    "learning_rate = 0.01\n",
    "n_units = 128\n",
    "vocab_size = 30_000\n",
    "batch_size = 64\n",
    "epochs = 100\n",
    "max_seq_len = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,(normal,simple) in enumerate(zip(normal_sentences,simple_sentences)):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. pairs before preprocessing: 167689\n",
      "No. pairs after preprocessing: 117952\n"
     ]
    }
   ],
   "source": [
    "# Remove instances where Simple Wikipedia and Wikipedia sentences are identical.\n",
    "identical_filter = (normal[2] != simple[2])\n",
    "\n",
    "# Define a regex pattern used to remove some wikipeida formatting artefacts.\n",
    "# pattern = r'(-lrb-)(?<=-lrb-)(.*?)(?=-rrb-)(-rrb-)'\n",
    "pattern = '-lrb-.*?-rrb-'\n",
    "\n",
    "# Select the column of the 'normal' dataframe with the relevant data.\n",
    "normal_sentences = normal[2][identical_filter]\n",
    "\n",
    "# For each sentence, we want to try to remove string artefacts from wikipedia, usually as a result of hyperlinks\n",
    "# These can greatly increase the size of our sequences, needlessly, and are also noise.\n",
    "input_texts=[]\n",
    "input_vocab=set()\n",
    "for text in normal_sentences:\n",
    "    text = text.lower()\n",
    "    matches = re.findall(pattern,text)\n",
    "    for match in matches:\n",
    "        match = \"\".join(match)\n",
    "        text = text.replace(match,\"\")\n",
    "    sentence = f'{text}'.split(' ')\n",
    "    input_texts.append(sentence)\n",
    "    \n",
    "    # Add unique words to vocabulary\n",
    "    for word in sentence:\n",
    "        if word not in input_vocab:\n",
    "            input_vocab.add(word)\n",
    "\n",
    "\n",
    "# Define beginning of sentence and end of sentence tokens.\n",
    "# These will allow us to initialize our decoder layer and also allow it to know when to stop.\n",
    "bos='bos '\n",
    "eos=' eos'\n",
    "\n",
    "# Do the same for sentences from simplified wikipedia.\n",
    "simplified_sentences = simple[2][identical_filter]\n",
    "target_texts=[]\n",
    "target_vocab=set()\n",
    "for text in normal_sentences:\n",
    "    text = text.lower()\n",
    "    matches = re.findall(pattern,text)\n",
    "    for match in matches:\n",
    "        match = \"\".join(match)\n",
    "        text = text.replace(match,\"\")\n",
    "    sentence = f'{bos}{text}{eos}'.split(' ')\n",
    "    target_texts.append(sentence)\n",
    "    for word in sentence:\n",
    "        if word not in target_vocab:\n",
    "            target_vocab.add(word)\n",
    "    \n",
    "print(f'No. pairs before preprocessing: {len(normal[2])}')\n",
    "print(f'No. pairs after preprocessing: {len(input_texts)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"asiatic cheetah  has for a long time been theoretically classified as a sub-species of the cheetah with the suffix `` venaticus '' applied at the end of its scientific binomial name acinonyx jubatus but at a cheetah reintroduction workshop organized in india on 9 september 2009 stephen j. o'brien from laboratory of genomic diversity of national cancer institute , usa who has in the past conducted numerous prestigious genetic studies including those on asiatic lions said that according to latest modern genetic studies which became possible only now it was discovered that in fact asiatic cheetah was genetically identical to the african cheetah with which it had separated only about 5000 years ago which was not enough time for a sub-species level differentiation , in comparison he said that the asian and african lion subspecies were separated some 100,000 years ago , so was the african and asian leopard subspecies 169,000 years ago . cheetah expert laurie marker of the cheetah conservation fund  and other wildlife experts assembled for the occasion advised the indian government that for reintroduction purposes india should source the cheetah from africa where they were much more numerous instead of trying to have some removed from the critically endangered low population of about worlds last 70 to 100 or so asiatic cheetahs left in iran .\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqs = [len(seq) for seq in input_texts]\n",
    "\" \".join(input_texts[seqs.index(max(seqs))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A wrapper for keras' tokenizer class which provides some much needed functionality.\n",
    "# Code from https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/21_Machine_Translation.ipynb\n",
    "class TokenizerWrap(Tokenizer):\n",
    "    \"\"\"Wrap the Tokenizer-class from Keras with more functionality.\"\"\"\n",
    "    def __init__(self, texts, padding,\n",
    "                 reverse=False, num_words=None,oov_token=None):\n",
    "        \"\"\"\n",
    "        :param texts: List of strings. This is the data-set.\n",
    "        :param padding: Either 'post' or 'pre' padding.\n",
    "        :param reverse: Boolean whether to reverse token-lists.\n",
    "        :param num_words: Max number of words to use.\n",
    "        \"\"\"\n",
    "        Tokenizer.__init__(self, num_words=num_words,oov_token=oov_token)\n",
    "        self.fit_on_texts(texts)\n",
    "        self.index_to_word = dict(zip(self.word_index.values(),\n",
    "                                      self.word_index.keys()))\n",
    "        self.tokens = self.texts_to_sequences(texts)\n",
    "        if reverse:\n",
    "            self.tokens = [list(reversed(x)) for x in self.tokens]\n",
    "            truncating = 'pre'\n",
    "        else:\n",
    "            truncating = 'post'\n",
    "        self.num_tokens = [len(x) for x in self.tokens]\n",
    "        self.max_tokens = np.mean(self.num_tokens) \\\n",
    "                          + 2 * np.std(self.num_tokens)\n",
    "        self.max_tokens = int(self.max_tokens)\n",
    "        self.tokens_padded = pad_sequences(self.tokens,\n",
    "                                           maxlen=self.max_tokens,\n",
    "                                           padding=padding,\n",
    "                                           truncating=truncating)\n",
    "    def token_to_word(self, token):\n",
    "        \"\"\"Lookup a single word from an integer-token.\"\"\"\n",
    "        word = \" \" if token == 0 else self.index_to_word[token]\n",
    "        return word \n",
    "    def tokens_to_string(self, tokens):\n",
    "        \"\"\"Convert a list of integer-tokens to a string.\"\"\"\n",
    "        words = [self.index_to_word[token]\n",
    "                 for token in tokens\n",
    "                 if token != 0]\n",
    "        text = \" \".join(words)\n",
    "        return text\n",
    "    def text_to_tokens(self, text, reverse=False, padding=False):\n",
    "        \"\"\"\n",
    "        Convert a single text-string to tokens with optional\n",
    "        reversal and padding.\n",
    "        \"\"\"\n",
    "        tokens = self.texts_to_sequences([text])\n",
    "        tokens = np.array(tokens)\n",
    "        if reverse:\n",
    "            tokens = np.flip(tokens, axis=1)\n",
    "            truncating = 'pre'\n",
    "        else:\n",
    "            truncating = 'post'\n",
    "        if padding:\n",
    "            tokens = pad_sequences(tokens,maxlen=self.max_tokens,\n",
    "                                   padding='pre',\n",
    "                                   truncating=truncating)\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.12 s, sys: 461 ms, total: 8.58 s\n",
      "Wall time: 9.26 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "input_tokenizer = TokenizerWrap(texts=input_texts,\n",
    "                                padding='pre',\n",
    "                                reverse=True,\n",
    "                                num_words=vocab_size,\n",
    "                                oov_token='unk')\n",
    "inputs_tokenized = input_tokenizer.tokens_padded\n",
    "inputs_tokenized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(117952, 54)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_tokenizer = TokenizerWrap(texts=target_texts,\n",
    "                                padding='post',\n",
    "                                reverse=False,\n",
    "                                num_words=vocab_size,\n",
    "                                oov_token='unk')\n",
    "targets_tokenized = target_tokenizer.tokens_padded\n",
    "targets_tokenized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(117952, 53)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input_data = inputs_tokenized\n",
    "decoder_input_data = targets_tokenized[:,:-1]\n",
    "decoder_input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 53, 30000)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_output_data = targets_tokenized[:,1:]\n",
    "decoder_output_list=[]\n",
    "for sequence in decoder_output_data[:batch_size]:\n",
    "    one_hot = to_categorical(sequence,num_classes=vocab_size)\n",
    "    decoder_output_list.append(one_hot)\n",
    "decoder_output_data = np.array(decoder_output_list).reshape(decoder_output_data[:batch_size].shape[0],decoder_output_data[:batch_size].shape[1],vocab_size)\n",
    "decoder_output_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400001 word vectors\n"
     ]
    }
   ],
   "source": [
    "embeddings_dict = {}\n",
    "f = open('./data/glove.6B/glove.6B.100d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:],dtype='float32')\n",
    "    embeddings_dict[word] = coefs\n",
    "f.close()\n",
    "print(f'Loaded {len(embeddings_dict)} word vectors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

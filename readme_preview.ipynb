{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blog Post: \n",
    "This project was completed as a capstone project for my General Assembly Data Science Immersive program. Coming from an academic background in physics, my contact with statistics and 'data science' had always been firmly in the realm of numbers, figures, curves, plots, etc. etc. So the notion of words being data was foreign to me. Natural language processing has since become an area of keen interest, and I decided to dedicate my capstone project to learning about the bleeding edge of natural language processing the only way a physics kid knows how; do a lil' swan dive into the research.\n",
    "\n",
    "The original inspiration for this capstone was the 'auto-tl;dr' bot on reddit. This automated reddit account reads articles and, through a process called text summarization (linguists are much better at coming up with understandable names for things than physicists are) it generates it's best guess at a summary for the contents therein, and posts it for those of us too busy or lazy to click through and read the original content. It had always intrigued me how well this bot had performed. Perhaps I could do something similar for research papers? This bot would have to do a good job of simplifying complex sentences. \n",
    "\n",
    "# Github README\n",
    "\n",
    "# Neural Machine Translation via Word-Level Seq2Seq with Attention for Text Simplification.\n",
    "\n",
    "# Abstract\n",
    "A Neural Machine Translation (NMT) system was implemented for the purposes of text simplification. The model is a word-level sequene-to-sequence (Seq2Seq) model with an attention mechanism. The Seq2Seq component\n",
    "\n",
    "# Introduction & Literature Review\n",
    "\n",
    "# Motivation\n",
    "\n",
    "# Methods\n",
    "\n",
    "# Results\n",
    "\n",
    "# Discussion\n",
    "\n",
    "# Summary\n",
    "The Seq2Seq model, invented by Google research scientist [] revolutionized the field of machine translation and firmly dominates many fields of sequence modelling interested in using deep learning.\n",
    "\n",
    "One of the larger areas that caught my attention was the intersection of natural language processing and deep learning. Naturally, I identified recurrent neural networks (RNNs) as a keen area of personal interest, and decided to focus my capstone project around using neural network architectures to simplify language and promote accessibility to information. This project/readme/blogpost/portfolio piece assumes a functional knowledge of some of the most popular neural network structures, functionality and limitations, although I will attempt to provide as many external links to outside sources of information as necessary to supplement the reader's understanding. Deep learning is a notoriously opaque topic, even for most data scientists!\n",
    "\n",
    "Text simplification requires sentence simplification, which in turn requires lexical simplification. The structure and intensity of the GA DSI course does not lend itself to a comprehensive literature review on text simplification, either in terms of demand for such solutions, previous approaches or an extensive and individually impressive contribution to the scientific literature on my part. This project uses recent advances in lexical simplification, and seeks to emulate similarly impressive research in sentence simplification, and finally it is my hope to offer an interactive platform for those interested to see these processes in action.\n",
    "\n",
    "\n",
    "Children, language learners and people language impairments such as dyslexia, aphasia, autism, etc. could benefit a lot from the increased access to information offered by text simplification. Additionally, science communication has historically been a difficult topic, requiring incredible charisma for a scientist to be able to rephrase their research in terms more approachable by the average person.\n",
    "\n",
    "Data Source: Paraphrase Database (PPDB 2.0)[http://paraphrase.org/#/download]\n",
    "\n",
    "Data Source: Wikipedia/Simplified Wikipedia Parallel Corpus [http://www.cs.pomona.edu/~dkauchak/simplification/]\n",
    "\n",
    "Embeddings: GloVe vs Word2Vec\n",
    "\n",
    "Encoder-Decoder: https://machinelearningmastery.com/develop-encoder-decoder-model-sequence-sequence-prediction-keras/\n",
    "https://towardsdatascience.com/neural-machine-translation-using-seq2seq-with-keras-c23540453c74\n",
    "\n",
    "code?? https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html\n",
    "\n",
    "other https://github.com/jbingel/rippletagger\n",
    "\n",
    "https://github.com/XingxingZhang/pysari\n",
    "https://github.com/cocoxu/simplification\n",
    "https://aclweb.org/anthology/C18-1021\n",
    "https://github.com/maciejkula/glove-python\n",
    "https://nlp.stanford.edu/projects/glove/\n",
    "https://www.aclweb.org/anthology/P14-1023\n",
    "http://cs.jhu.edu/~napoles/res/tacl2016-optimizing.pdf\n",
    "\n",
    "\n",
    "https://www.aclweb.org/anthology/I17-1030\n",
    "https://www.aclweb.org/anthology/P14-2075\n",
    "https://www.aclweb.org/anthology/P15-2011\n",
    "https://www.aclweb.org/anthology/P14-1023\n",
    "http://cis.upenn.edu/~ccb/publications/simple-ppdb.pdf\n",
    "https://cs.brown.edu/people/epavlick/papers/ppdb2.pdf\n",
    "http://cs.jhu.edu/~napoles/res/tacl2016-optimizing.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

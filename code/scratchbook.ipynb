{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import string\n",
    "import pickle\n",
    "import operator\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Embedding, LSTM, Dropout, RepeatVector, Bidirectional\n",
    "from keras.layers import GRU, Input, Dense, TimeDistributed, Activation, merge\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.engine import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import sparse_categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cherokee, Oklahoma</td>\n",
       "      <td>0</td>\n",
       "      <td>It is the county seat of Alfalfa County .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cherokee, Oklahoma</td>\n",
       "      <td>0</td>\n",
       "      <td>Cherokee is a city in Alfalfa County , Oklahom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Skateboard</td>\n",
       "      <td>5</td>\n",
       "      <td>Skateboard decks are usually between 28 and 33...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Skateboard</td>\n",
       "      <td>5</td>\n",
       "      <td>The underside of the deck can be printed with ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Skateboard</td>\n",
       "      <td>6</td>\n",
       "      <td>This was created by two surfers ; Ben Whatson ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    0  1                                                  2\n",
       "0  Cherokee, Oklahoma  0          It is the county seat of Alfalfa County .\n",
       "1  Cherokee, Oklahoma  0  Cherokee is a city in Alfalfa County , Oklahom...\n",
       "2          Skateboard  5  Skateboard decks are usually between 28 and 33...\n",
       "3          Skateboard  5  The underside of the deck can be printed with ...\n",
       "4          Skateboard  6  This was created by two surfers ; Ben Whatson ..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal = pd.read_csv('../data/sentence-aligned.v2/normal.aligned',sep='\\t',header=None)\n",
    "normal.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cherokee, Oklahoma</td>\n",
       "      <td>0</td>\n",
       "      <td>It is the county seat of Alfalfa County .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cherokee, Oklahoma</td>\n",
       "      <td>0</td>\n",
       "      <td>Cherokee is a city of Oklahoma in the United S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Skateboard</td>\n",
       "      <td>2</td>\n",
       "      <td>Skateboard decks are normally between 28 and 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Skateboard</td>\n",
       "      <td>2</td>\n",
       "      <td>The bottom of the deck can be printed with a d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Skateboard</td>\n",
       "      <td>3</td>\n",
       "      <td>The longboard was made by two surfers ; Ben Wh...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    0  1                                                  2\n",
       "0  Cherokee, Oklahoma  0          It is the county seat of Alfalfa County .\n",
       "1  Cherokee, Oklahoma  0  Cherokee is a city of Oklahoma in the United S...\n",
       "2          Skateboard  2  Skateboard decks are normally between 28 and 3...\n",
       "3          Skateboard  2  The bottom of the deck can be printed with a d...\n",
       "4          Skateboard  3  The longboard was made by two surfers ; Ben Wh..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple = pd.read_csv('../data/sentence-aligned.v2/simple.aligned',sep='\\t',header=None)\n",
    "simple.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128  # Batch size for training.\n",
    "epochs = 100  # Number of epochs to train for.\n",
    "latent_dim = 300  # Latent dimensionality of the encoding space.\n",
    "num_samples = 10_000  # Number of samples to train on.  ########### SHORTENED FOR PRACTICE PURPOSES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify sentences in the parallel corpus that were identical. This reduces the size of our training data by about 50 thousand. Another step of preprocessing is to turn everything to lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "identical_filter = (normal[2] != simple[2])\n",
    "input_texts = normal[2][identical_filter]\n",
    "input_texts = np.array([f'bos {text} eos'.lower().split(' ') for text in input_texts])\n",
    "target_texts = simple[2][identical_filter]\n",
    "target_texts = np.array([f'bos {text} eos'.lower().split(' ') for text in target_texts])\n",
    "print(f'No. pairs before preprocessing: {len(normal[2])}')\n",
    "print(f'No. pairs after preprocessing: {len(input_texts)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 117952\n",
      "Number of unique input tokens: 121211\n",
      "Number of unique output tokens: 105709\n",
      "Max sequence length for inputs: 236\n",
      "Max sequence length for outputs: 192\n"
     ]
    }
   ],
   "source": [
    "input_words = set()\n",
    "target_words = set()\n",
    "for i,sentence in enumerate(input_texts[:min(num_samples,len(input_texts)-1)]):\n",
    "    for word in input_texts[i]:\n",
    "        if word not in input_words:\n",
    "            input_words.add(word)\n",
    "    for word in target_texts[i]:\n",
    "        if word not in target_words:\n",
    "            target_words.add(word)\n",
    "            \n",
    "input_words = sorted(list(input_words))\n",
    "target_words = sorted(list(target_words))\n",
    "num_encoder_tokens = len(input_words)\n",
    "num_decoder_tokens = len(target_words)\n",
    "max_encoder_seq_length = max([len(sen) for sen in input_texts])\n",
    "max_decoder_seq_length = max([len(sen) for sen in target_texts])\n",
    "\n",
    "print('Number of samples:', len(input_texts))\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output tokens:', num_decoder_tokens)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our input vocabulary is about twelve thousand words, whereas the output vocabulary is about ten thousand. This makes sense; we expect the simplified vocabulary to be smaller."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NUM_WORDS=100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(input_texts)\n",
    "input_sequences = tokenizer.texts_to_sequences(input_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117952"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7323, 8, 7, 44, 5, 29620, 62, 2, 893, 2, 50, 55, 3]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "print(len(input_sequences[0]))\n",
    "print(len(input_texts[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7323, 8, 7, 44, 4, 893, 5, 1, 50, 55, 3]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_sequences = tokenizer.texts_to_sequences(target_texts)\n",
    "output_sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cherokee',\n",
       " 'is',\n",
       " 'a',\n",
       " 'city',\n",
       " 'of',\n",
       " 'oklahoma',\n",
       " 'in',\n",
       " 'the',\n",
       " 'united',\n",
       " 'states',\n",
       " '.']"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "print(len(output_sequences[0]))\n",
    "print(len(target_texts[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 1,\n",
       " ',': 2,\n",
       " '.': 3,\n",
       " 'of': 4,\n",
       " 'in': 5,\n",
       " 'and': 6,\n",
       " 'a': 7,\n",
       " 'is': 8,\n",
       " 'to': 9,\n",
       " '-lrb-': 10,\n",
       " '-rrb-': 11,\n",
       " 'was': 12,\n",
       " 'as': 13,\n",
       " 'for': 14,\n",
       " 'on': 15,\n",
       " 'by': 16,\n",
       " '``': 17,\n",
       " 'with': 18,\n",
       " \"''\": 19,\n",
       " \"'s\": 20,\n",
       " 'that': 21,\n",
       " 'from': 22,\n",
       " 'it': 23,\n",
       " 'an': 24,\n",
       " 'at': 25,\n",
       " 'or': 26,\n",
       " 'are': 27,\n",
       " 'his': 28,\n",
       " 'he': 29,\n",
       " 'which': 30,\n",
       " 'also': 31,\n",
       " 'be': 32,\n",
       " ';': 33,\n",
       " 'one': 34,\n",
       " 'first': 35,\n",
       " 'has': 36,\n",
       " 'born': 37,\n",
       " 'were': 38,\n",
       " 'who': 39,\n",
       " 'its': 40,\n",
       " 'france': 41,\n",
       " 'this': 42,\n",
       " 'â': 43,\n",
       " 'city': 44,\n",
       " 'known': 45,\n",
       " ':': 46,\n",
       " 'department': 47,\n",
       " 'commune': 48,\n",
       " 'not': 49,\n",
       " 'united': 50,\n",
       " 'after': 51,\n",
       " 'have': 52,\n",
       " 'but': 53,\n",
       " 'their': 54,\n",
       " 'states': 55,\n",
       " 'other': 56,\n",
       " 'new': 57,\n",
       " 'had': 58,\n",
       " 'most': 59,\n",
       " 'two': 60,\n",
       " 'they': 61,\n",
       " 'county': 62,\n",
       " 'her': 63,\n",
       " 'been': 64,\n",
       " 'world': 65,\n",
       " 'football': 66,\n",
       " 'used': 67,\n",
       " 'when': 68,\n",
       " 'may': 69,\n",
       " 'american': 70,\n",
       " 'into': 71,\n",
       " 'region': 72,\n",
       " 'such': 73,\n",
       " 'all': 74,\n",
       " 'can': 75,\n",
       " 'more': 76,\n",
       " 'during': 77,\n",
       " 'she': 78,\n",
       " \"'\": 79,\n",
       " 'name': 80,\n",
       " 'some': 81,\n",
       " 'between': 82,\n",
       " 'national': 83,\n",
       " 'only': 84,\n",
       " 'time': 85,\n",
       " 'state': 86,\n",
       " 'many': 87,\n",
       " 'called': 88,\n",
       " 'than': 89,\n",
       " 'north': 90,\n",
       " 'part': 91,\n",
       " 'there': 92,\n",
       " 'located': 93,\n",
       " 'over': 94,\n",
       " 'about': 95,\n",
       " 'south': 96,\n",
       " 'team': 97,\n",
       " 'where': 98,\n",
       " 'became': 99,\n",
       " 'years': 100,\n",
       " 'three': 101,\n",
       " 'former': 102,\n",
       " 'while': 103,\n",
       " 'series': 104,\n",
       " 'player': 105,\n",
       " 'made': 106,\n",
       " 'ã': 107,\n",
       " 'de': 108,\n",
       " 'later': 109,\n",
       " 'district': 110,\n",
       " 'second': 111,\n",
       " 'town': 112,\n",
       " 'area': 113,\n",
       " 'year': 114,\n",
       " 'university': 115,\n",
       " '1': 116,\n",
       " 'including': 117,\n",
       " 'september': 118,\n",
       " 'under': 119,\n",
       " 'would': 120,\n",
       " 'both': 121,\n",
       " 'being': 122,\n",
       " 'since': 123,\n",
       " 'english': 124,\n",
       " 'until': 125,\n",
       " '--': 126,\n",
       " 'league': 127,\n",
       " 'northern': 128,\n",
       " 'british': 129,\n",
       " 'game': 130,\n",
       " 'october': 131,\n",
       " 'january': 132,\n",
       " 'people': 133,\n",
       " 'often': 134,\n",
       " 'then': 135,\n",
       " 'these': 136,\n",
       " 'july': 137,\n",
       " 'august': 138,\n",
       " 'june': 139,\n",
       " 'music': 140,\n",
       " 'west': 141,\n",
       " 'well': 142,\n",
       " 'up': 143,\n",
       " 'march': 144,\n",
       " 'number': 145,\n",
       " '2': 146,\n",
       " 'river': 147,\n",
       " 'through': 148,\n",
       " 'released': 149,\n",
       " 'him': 150,\n",
       " 'november': 151,\n",
       " '`': 152,\n",
       " 'named': 153,\n",
       " 'use': 154,\n",
       " 'system': 155,\n",
       " 'film': 156,\n",
       " 'december': 157,\n",
       " 'before': 158,\n",
       " 'however': 159,\n",
       " 'several': 160,\n",
       " 'east': 161,\n",
       " 'april': 162,\n",
       " 'family': 163,\n",
       " 'war': 164,\n",
       " 'school': 165,\n",
       " 'japanese': 166,\n",
       " 'season': 167,\n",
       " 'february': 168,\n",
       " 'currently': 169,\n",
       " 'band': 170,\n",
       " 'major': 171,\n",
       " 'no': 172,\n",
       " 'out': 173,\n",
       " 'ð': 174,\n",
       " 'group': 175,\n",
       " 'early': 176,\n",
       " 'best': 177,\n",
       " '-': 178,\n",
       " 'played': 179,\n",
       " 'german': 180,\n",
       " 'century': 181,\n",
       " 'although': 182,\n",
       " '1\\\\/4': 183,\n",
       " 'london': 184,\n",
       " 'professional': 185,\n",
       " 'will': 186,\n",
       " 'based': 187,\n",
       " 'album': 188,\n",
       " 'î': 189,\n",
       " 'now': 190,\n",
       " 'each': 191,\n",
       " 'four': 192,\n",
       " 'form': 193,\n",
       " 'footballer': 194,\n",
       " 'species': 195,\n",
       " 'international': 196,\n",
       " 'i': 197,\n",
       " 'any': 198,\n",
       " 'province': 199,\n",
       " 'england': 200,\n",
       " 'them': 201,\n",
       " 'john': 202,\n",
       " 'because': 203,\n",
       " '2007': 204,\n",
       " 'population': 205,\n",
       " 'island': 206,\n",
       " 'french': 207,\n",
       " 'same': 208,\n",
       " '2008': 209,\n",
       " 'government': 210,\n",
       " 'largest': 211,\n",
       " 'around': 212,\n",
       " 'club': 213,\n",
       " 'municipality': 214,\n",
       " 'found': 215,\n",
       " 'following': 216,\n",
       " 'large': 217,\n",
       " 'king': 218,\n",
       " 'southern': 219,\n",
       " 'plays': 220,\n",
       " 'small': 221,\n",
       " '%': 222,\n",
       " 'so': 223,\n",
       " 'work': 224,\n",
       " '3': 225,\n",
       " 'central': 226,\n",
       " 'won': 227,\n",
       " '2006': 228,\n",
       " 'president': 229,\n",
       " 'if': 230,\n",
       " 'main': 231,\n",
       " 'center': 232,\n",
       " 'day': 233,\n",
       " 'home': 234,\n",
       " 'high': 235,\n",
       " 'capital': 236,\n",
       " 'western': 237,\n",
       " 'york': 238,\n",
       " 'within': 239,\n",
       " 'eastern': 240,\n",
       " '4': 241,\n",
       " 'began': 242,\n",
       " 'line': 243,\n",
       " 'television': 244,\n",
       " 'against': 245,\n",
       " '2009': 246,\n",
       " 'la': 247,\n",
       " '10': 248,\n",
       " 'common': 249,\n",
       " 'along': 250,\n",
       " 'great': 251,\n",
       " 'usually': 252,\n",
       " 'tropical': 253,\n",
       " '5': 254,\n",
       " 'near': 255,\n",
       " 'games': 256,\n",
       " 'water': 257,\n",
       " 'term': 258,\n",
       " '2010': 259,\n",
       " 'show': 260,\n",
       " 'death': 261,\n",
       " 'like': 262,\n",
       " '2005': 263,\n",
       " 'company': 264,\n",
       " 'include': 265,\n",
       " 'championship': 266,\n",
       " 'published': 267,\n",
       " 'party': 268,\n",
       " 'rock': 269,\n",
       " 'sometimes': 270,\n",
       " 'place': 271,\n",
       " 'million': 272,\n",
       " 'ii': 273,\n",
       " 'last': 274,\n",
       " 'germany': 275,\n",
       " 'japan': 276,\n",
       " 'life': 277,\n",
       " 'member': 278,\n",
       " 'end': 279,\n",
       " 'song': 280,\n",
       " 'public': 281,\n",
       " 'formed': 282,\n",
       " 'u.s.': 283,\n",
       " 'third': 284,\n",
       " 'very': 285,\n",
       " 'long': 286,\n",
       " 'general': 287,\n",
       " 'hurricane': 288,\n",
       " 'single': 289,\n",
       " '6': 290,\n",
       " 'language': 291,\n",
       " '15': 292,\n",
       " '2000': 293,\n",
       " 'another': 294,\n",
       " 'modern': 295,\n",
       " 'country': 296,\n",
       " 'australia': 297,\n",
       " 'book': 298,\n",
       " 'kingdom': 299,\n",
       " 'different': 300,\n",
       " 'held': 301,\n",
       " 'developed': 302,\n",
       " '#': 303,\n",
       " 'produced': 304,\n",
       " 'much': 305,\n",
       " 'house': 306,\n",
       " '20': 307,\n",
       " 'referred': 308,\n",
       " 'period': 309,\n",
       " 'title': 310,\n",
       " 'members': 311,\n",
       " 'founded': 312,\n",
       " 'see': 313,\n",
       " 'due': 314,\n",
       " 'set': 315,\n",
       " 'storm': 316,\n",
       " 'n': 317,\n",
       " 'history': 318,\n",
       " 'original': 319,\n",
       " 'five': 320,\n",
       " 'hockey': 321,\n",
       " 'm': 322,\n",
       " 'commonly': 323,\n",
       " 'park': 324,\n",
       " 'america': 325,\n",
       " '30': 326,\n",
       " 'church': 327,\n",
       " 'written': 328,\n",
       " 'sea': 329,\n",
       " 'considered': 330,\n",
       " 's': 331,\n",
       " 'old': 332,\n",
       " 'own': 333,\n",
       " 'italian': 334,\n",
       " 'though': 335,\n",
       " 'union': 336,\n",
       " 'become': 337,\n",
       " 'do': 338,\n",
       " 'switzerland': 339,\n",
       " 'age': 340,\n",
       " 'red': 341,\n",
       " '12': 342,\n",
       " 'originally': 343,\n",
       " '7': 344,\n",
       " 'pas-de-calais': 345,\n",
       " 'college': 346,\n",
       " 'order': 347,\n",
       " 'son': 348,\n",
       " 'back': 349,\n",
       " 'those': 350,\n",
       " 'station': 351,\n",
       " 'still': 352,\n",
       " 'father': 353,\n",
       " 'late': 354,\n",
       " '11': 355,\n",
       " 'among': 356,\n",
       " 'created': 357,\n",
       " 'children': 358,\n",
       " 'association': 359,\n",
       " 'did': 360,\n",
       " 'left': 361,\n",
       " '8': 362,\n",
       " 'given': 363,\n",
       " 'popular': 364,\n",
       " 'built': 365,\n",
       " 'council': 366,\n",
       " '25': 367,\n",
       " 'royal': 368,\n",
       " 'canton': 369,\n",
       " 'according': 370,\n",
       " 'video': 371,\n",
       " 'even': 372,\n",
       " 'aisne': 373,\n",
       " 'picardy': 374,\n",
       " 'republic': 375,\n",
       " '16': 376,\n",
       " '13': 377,\n",
       " '2004': 378,\n",
       " 'nord-pas-de-calais': 379,\n",
       " 'could': 380,\n",
       " 'using': 381,\n",
       " 'what': 382,\n",
       " 'served': 383,\n",
       " 'took': 384,\n",
       " '18': 385,\n",
       " 'james': 386,\n",
       " 'à': 387,\n",
       " 'northwestern': 388,\n",
       " 'local': 389,\n",
       " 'white': 390,\n",
       " 'land': 391,\n",
       " 'times': 392,\n",
       " 'playing': 393,\n",
       " 'power': 394,\n",
       " 'grand': 395,\n",
       " '24': 396,\n",
       " 'black': 397,\n",
       " '14': 398,\n",
       " 'europe': 399,\n",
       " 'ø': 400,\n",
       " '9': 401,\n",
       " 'important': 402,\n",
       " '2001': 403,\n",
       " 'died': 404,\n",
       " 'final': 405,\n",
       " 'role': 406,\n",
       " 'just': 407,\n",
       " '17': 408,\n",
       " 'having': 409,\n",
       " '2003': 410,\n",
       " 'body': 411,\n",
       " 'make': 412,\n",
       " 'formula': 413,\n",
       " 'example': 414,\n",
       " 'match': 415,\n",
       " 'established': 416,\n",
       " 'led': 417,\n",
       " 'countries': 418,\n",
       " 'live': 419,\n",
       " 'word': 420,\n",
       " 'point': 421,\n",
       " '21': 422,\n",
       " '&': 423,\n",
       " 'service': 424,\n",
       " 'current': 425,\n",
       " 'william': 426,\n",
       " 'european': 427,\n",
       " 'india': 428,\n",
       " 'greek': 429,\n",
       " 'event': 430,\n",
       " 'moved': 431,\n",
       " 'calvados': 432,\n",
       " 'wrestling': 433,\n",
       " 'official': 434,\n",
       " 'basse-normandie': 435,\n",
       " '23': 436,\n",
       " '19': 437,\n",
       " 'days': 438,\n",
       " 'division': 439,\n",
       " 'islands': 440,\n",
       " 'works': 441,\n",
       " 'air': 442,\n",
       " 'included': 443,\n",
       " 'without': 444,\n",
       " 'roman': 445,\n",
       " 'generally': 446,\n",
       " 'play': 447,\n",
       " 'e': 448,\n",
       " 'record': 449,\n",
       " 'coast': 450,\n",
       " 'award': 451,\n",
       " 'version': 452,\n",
       " 'human': 453,\n",
       " 'head': 454,\n",
       " 'way': 455,\n",
       " 'top': 456,\n",
       " 'spanish': 457,\n",
       " 'received': 458,\n",
       " 'various': 459,\n",
       " 'six': 460,\n",
       " 'km': 461,\n",
       " 'similar': 462,\n",
       " 'young': 463,\n",
       " 'simply': 464,\n",
       " 'ï': 465,\n",
       " 'lake': 466,\n",
       " 'george': 467,\n",
       " 'california': 468,\n",
       " 'village': 469,\n",
       " 'political': 470,\n",
       " 'ice': 471,\n",
       " '2002': 472,\n",
       " 'law': 473,\n",
       " 'few': 474,\n",
       " 'å': 475,\n",
       " '22': 476,\n",
       " 'iowa': 477,\n",
       " 'areas': 478,\n",
       " 'you': 479,\n",
       " 'short': 480,\n",
       " 'together': 481,\n",
       " 'famous': 482,\n",
       " '26': 483,\n",
       " 'army': 484,\n",
       " 'development': 485,\n",
       " 'cup': 486,\n",
       " 'australian': 487,\n",
       " 'character': 488,\n",
       " 'free': 489,\n",
       " 'pakistan': 490,\n",
       " 'off': 491,\n",
       " 'retired': 492,\n",
       " 'career': 493,\n",
       " 'wwe': 494,\n",
       " 'today': 495,\n",
       " 'making': 496,\n",
       " 'throughout': 497,\n",
       " '28': 498,\n",
       " 'side': 499,\n",
       " 'railway': 500,\n",
       " '27': 501,\n",
       " 'total': 502,\n",
       " 'canada': 503,\n",
       " 'office': 504,\n",
       " 'empire': 505,\n",
       " 'women': 506,\n",
       " 'minister': 507,\n",
       " 'again': 508,\n",
       " 'list': 509,\n",
       " 'art': 510,\n",
       " 'actor': 511,\n",
       " 'natural': 512,\n",
       " 'road': 513,\n",
       " '$': 514,\n",
       " 'earth': 515,\n",
       " 'said': 516,\n",
       " 'canadian': 517,\n",
       " 'force': 518,\n",
       " 'either': 519,\n",
       " '29': 520,\n",
       " 'singer': 521,\n",
       " 'south-western': 522,\n",
       " 'discovered': 523,\n",
       " 'does': 524,\n",
       " 'charles': 525,\n",
       " 'ñ': 526,\n",
       " 'mother': 527,\n",
       " 'florida': 528,\n",
       " 'st.': 529,\n",
       " 'energy': 530,\n",
       " 'brazilian': 531,\n",
       " 'novel': 532,\n",
       " 'result': 533,\n",
       " 'research': 534,\n",
       " 'others': 535,\n",
       " 'down': 536,\n",
       " 'hall': 537,\n",
       " 'came': 538,\n",
       " 'ù': 539,\n",
       " 'class': 540,\n",
       " 'little': 541,\n",
       " 'once': 542,\n",
       " 'san': 543,\n",
       " 'next': 544,\n",
       " 'thus': 545,\n",
       " 'wife': 546,\n",
       " 'science': 547,\n",
       " 'stadium': 548,\n",
       " 'parts': 549,\n",
       " 'includes': 550,\n",
       " 'race': 551,\n",
       " 'meaning': 552,\n",
       " 'network': 553,\n",
       " 'census': 554,\n",
       " 'radio': 555,\n",
       " 'right': 556,\n",
       " 'atlantic': 557,\n",
       " 'director': 558,\n",
       " 'gironde': 559,\n",
       " 'every': 560,\n",
       " 'upon': 561,\n",
       " 'seat': 562,\n",
       " 'man': 563,\n",
       " 'site': 564,\n",
       " '1999': 565,\n",
       " 'announced': 566,\n",
       " 'battle': 567,\n",
       " 'key': 568,\n",
       " 'light': 569,\n",
       " 'lead': 570,\n",
       " 'christian': 571,\n",
       " 'started': 572,\n",
       " 'married': 573,\n",
       " 'military': 574,\n",
       " 'officially': 575,\n",
       " 'range': 576,\n",
       " 'author': 577,\n",
       " 'open': 578,\n",
       " 'racing': 579,\n",
       " 'present': 580,\n",
       " 'appeared': 581,\n",
       " 'ancient': 582,\n",
       " 'less': 583,\n",
       " 'systems': 584,\n",
       " 'community': 585,\n",
       " 'queen': 586,\n",
       " 'metal': 587,\n",
       " 'take': 588,\n",
       " 'seven': 589,\n",
       " 'africa': 590,\n",
       " 'china': 591,\n",
       " 'type': 592,\n",
       " 'across': 593,\n",
       " 'act': 594,\n",
       " 'means': 595,\n",
       " 'living': 596,\n",
       " 'field': 597,\n",
       " 'special': 598,\n",
       " 'approximately': 599,\n",
       " 'star': 600,\n",
       " 'green': 601,\n",
       " 'italy': 602,\n",
       " 'street': 603,\n",
       " 'recorded': 604,\n",
       " 'night': 605,\n",
       " 'described': 606,\n",
       " 'independent': 607,\n",
       " 'organization': 608,\n",
       " 'records': 609,\n",
       " 'full': 610,\n",
       " 'further': 611,\n",
       " 'wrote': 612,\n",
       " 'release': 613,\n",
       " 'production': 614,\n",
       " 'position': 615,\n",
       " 'valley': 616,\n",
       " 'players': 617,\n",
       " 'itself': 618,\n",
       " 'taken': 619,\n",
       " 'daughter': 620,\n",
       " 'leader': 621,\n",
       " 'standard': 622,\n",
       " 'fourth': 623,\n",
       " 'emperor': 624,\n",
       " 'never': 625,\n",
       " 'middle': 626,\n",
       " 'whose': 627,\n",
       " 'control': 628,\n",
       " 'actress': 629,\n",
       " 'person': 630,\n",
       " 'ireland': 631,\n",
       " 'story': 632,\n",
       " 'orchestra': 633,\n",
       " 'david': 634,\n",
       " 'almost': 635,\n",
       " 'entertainment': 636,\n",
       " '100': 637,\n",
       " 'especially': 638,\n",
       " 'airport': 639,\n",
       " 'windows': 640,\n",
       " 'forms': 641,\n",
       " 'better': 642,\n",
       " 'parliament': 643,\n",
       " '1997': 644,\n",
       " 'building': 645,\n",
       " 'henry': 646,\n",
       " 'men': 647,\n",
       " 'should': 648,\n",
       " 'names': 649,\n",
       " 'food': 650,\n",
       " 'prime': 651,\n",
       " 'movement': 652,\n",
       " 'books': 653,\n",
       " 'scotland': 654,\n",
       " 'god': 655,\n",
       " 'introduced': 656,\n",
       " 'color': 657,\n",
       " 'traditional': 658,\n",
       " 'paul': 659,\n",
       " 'studio': 660,\n",
       " 'k': 661,\n",
       " 'o': 662,\n",
       " '1998': 663,\n",
       " 'car': 664,\n",
       " 'went': 665,\n",
       " 'aquitaine': 666,\n",
       " 'study': 667,\n",
       " 'latin': 668,\n",
       " 'uk': 669,\n",
       " 'program': 670,\n",
       " 'space': 671,\n",
       " 'robert': 672,\n",
       " 'information': 673,\n",
       " 'rather': 674,\n",
       " 'lost': 675,\n",
       " 'brother': 676,\n",
       " 'teams': 677,\n",
       " 'dã': 678,\n",
       " 'chinese': 679,\n",
       " 'chemical': 680,\n",
       " '31': 681,\n",
       " 'forces': 682,\n",
       " 'debut': 683,\n",
       " 'support': 684,\n",
       " 'sports': 685,\n",
       " 'features': 686,\n",
       " 'events': 687,\n",
       " 'summer': 688,\n",
       " 'services': 689,\n",
       " 'himself': 690,\n",
       " 'must': 691,\n",
       " 'manager': 692,\n",
       " 'iii': 693,\n",
       " 'social': 694,\n",
       " 'sold': 695,\n",
       " 'mario': 696,\n",
       " 'champion': 697,\n",
       " 'process': 698,\n",
       " 'continued': 699,\n",
       " 'software': 700,\n",
       " 'russian': 701,\n",
       " 'category': 702,\n",
       " 'least': 703,\n",
       " 'leading': 704,\n",
       " 'calendar': 705,\n",
       " 'indian': 706,\n",
       " 'miles': 707,\n",
       " '\\\\/': 708,\n",
       " '1996': 709,\n",
       " '1980': 710,\n",
       " 'bank': 711,\n",
       " 'outside': 712,\n",
       " 'instead': 713,\n",
       " 'refers': 714,\n",
       " 'addition': 715,\n",
       " 'design': 716,\n",
       " 'southeastern': 717,\n",
       " 'formerly': 718,\n",
       " 'scottish': 719,\n",
       " 'super': 720,\n",
       " 'bc': 721,\n",
       " 'contains': 722,\n",
       " 'level': 723,\n",
       " 'available': 724,\n",
       " 'civil': 725,\n",
       " 'c.': 726,\n",
       " 'stage': 727,\n",
       " 'prize': 728,\n",
       " 'ain': 729,\n",
       " 'duke': 730,\n",
       " 'seen': 731,\n",
       " '1981': 732,\n",
       " 'court': 733,\n",
       " 'half': 734,\n",
       " 'child': 735,\n",
       " 'theory': 736,\n",
       " 'performed': 737,\n",
       " 'followed': 738,\n",
       " '1979': 739,\n",
       " 'bay': 740,\n",
       " 'eventually': 741,\n",
       " 'signed': 742,\n",
       " 'consists': 743,\n",
       " 'ocean': 744,\n",
       " 'society': 745,\n",
       " 'computer': 746,\n",
       " 'plants': 747,\n",
       " 'larger': 748,\n",
       " 'associated': 749,\n",
       " 'territory': 750,\n",
       " 'replaced': 751,\n",
       " 'designed': 752,\n",
       " 'lower': 753,\n",
       " 'highest': 754,\n",
       " 'low': 755,\n",
       " 'model': 756,\n",
       " 'related': 757,\n",
       " '!': 758,\n",
       " 'nhl': 759,\n",
       " 'style': 760,\n",
       " '1\\\\/2': 761,\n",
       " 'dutch': 762,\n",
       " 'run': 763,\n",
       " 'mexico': 764,\n",
       " '1994': 765,\n",
       " 'minor': 766,\n",
       " 'per': 767,\n",
       " 'michael': 768,\n",
       " '1986': 769,\n",
       " 'good': 770,\n",
       " 'data': 771,\n",
       " '1974': 772,\n",
       " 'paris': 773,\n",
       " 'episode': 774,\n",
       " 'months': 775,\n",
       " '3\\\\/4': 776,\n",
       " 'wales': 777,\n",
       " 'typically': 778,\n",
       " 'surface': 779,\n",
       " 'size': 780,\n",
       " 'beginning': 781,\n",
       " 'us': 782,\n",
       " 'how': 783,\n",
       " 'opened': 784,\n",
       " 'politician': 785,\n",
       " 'prince': 786,\n",
       " 'plant': 787,\n",
       " 'los': 788,\n",
       " 'baseball': 789,\n",
       " 'c': 790,\n",
       " 'awards': 791,\n",
       " '1990': 792,\n",
       " 'above': 793,\n",
       " 'greater': 794,\n",
       " 'louis': 795,\n",
       " 'terms': 796,\n",
       " 'prix': 797,\n",
       " '1992': 798,\n",
       " '1995': 799,\n",
       " 'britain': 800,\n",
       " 'north-western': 801,\n",
       " 'returned': 802,\n",
       " 'remained': 803,\n",
       " 'rights': 804,\n",
       " 'successful': 805,\n",
       " 'port': 806,\n",
       " 'close': 807,\n",
       " 'berlin': 808,\n",
       " 'worked': 809,\n",
       " 'working': 810,\n",
       " 'genus': 811,\n",
       " 'asia': 812,\n",
       " 'project': 813,\n",
       " 'host': 814,\n",
       " 'virginia': 815,\n",
       " 'festival': 816,\n",
       " 'animals': 817,\n",
       " '1976': 818,\n",
       " 'love': 819,\n",
       " 'opera': 820,\n",
       " '1993': 821,\n",
       " 'blue': 822,\n",
       " 'nintendo': 823,\n",
       " 'strong': 824,\n",
       " 'case': 825,\n",
       " 'soon': 826,\n",
       " 'change': 827,\n",
       " 'gave': 828,\n",
       " 'peter': 829,\n",
       " 'widely': 830,\n",
       " 'far': 831,\n",
       " 'changed': 832,\n",
       " 'below': 833,\n",
       " 'awarded': 834,\n",
       " 'square': 835,\n",
       " 'washington': 836,\n",
       " 'caused': 837,\n",
       " 'native': 838,\n",
       " 'despite': 839,\n",
       " 'cities': 840,\n",
       " 'songs': 841,\n",
       " 'education': 842,\n",
       " 'source': 843,\n",
       " 'away': 844,\n",
       " 'groups': 845,\n",
       " 'numbers': 846,\n",
       " 'pacific': 847,\n",
       " '1982': 848,\n",
       " 'sir': 849,\n",
       " 'uses': 850,\n",
       " 'chief': 851,\n",
       " 'page': 852,\n",
       " 'writer': 853,\n",
       " 'languages': 854,\n",
       " 'gold': 855,\n",
       " 'wikipedia': 856,\n",
       " 'artist': 857,\n",
       " 'disney': 858,\n",
       " 'private': 859,\n",
       " 'previously': 860,\n",
       " 'catholic': 861,\n",
       " 'separate': 862,\n",
       " 'pokã': 863,\n",
       " 'primary': 864,\n",
       " '1975': 865,\n",
       " 'texas': 866,\n",
       " 'composed': 867,\n",
       " 'divided': 868,\n",
       " 'joined': 869,\n",
       " 'federal': 870,\n",
       " 'films': 871,\n",
       " 'ring': 872,\n",
       " 'richard': 873,\n",
       " 'situated': 874,\n",
       " '1991': 875,\n",
       " 'length': 876,\n",
       " 'success': 877,\n",
       " 'particularly': 878,\n",
       " '1977': 879,\n",
       " 'starting': 880,\n",
       " 'track': 881,\n",
       " '1984': 882,\n",
       " 'significant': 883,\n",
       " 'mostly': 884,\n",
       " 'cents': 885,\n",
       " '1989': 886,\n",
       " 'units': 887,\n",
       " 'rail': 888,\n",
       " 'thomas': 889,\n",
       " 'material': 890,\n",
       " 'whom': 891,\n",
       " 'cells': 892,\n",
       " 'oklahoma': 893,\n",
       " 'pays': 894,\n",
       " 'tour': 895,\n",
       " 'raw': 896,\n",
       " 'win': 897,\n",
       " 'fire': 898,\n",
       " 'articles': 899,\n",
       " 'chicago': 900,\n",
       " 'elected': 901,\n",
       " '1983': 902,\n",
       " '1970': 903,\n",
       " 'points': 904,\n",
       " 'average': 905,\n",
       " 'initially': 906,\n",
       " 'borough': 907,\n",
       " 'sun': 908,\n",
       " 'primarily': 909,\n",
       " 'administrative': 910,\n",
       " 'metropolitan': 911,\n",
       " 'police': 912,\n",
       " 'winter': 913,\n",
       " 'performance': 914,\n",
       " 'loire': 915,\n",
       " '1978': 916,\n",
       " 'lines': 917,\n",
       " 'angeles': 918,\n",
       " 'ten': 919,\n",
       " 'secretary': 920,\n",
       " 'mountain': 921,\n",
       " 'engine': 922,\n",
       " 'types': 923,\n",
       " '1987': 924,\n",
       " 'smaller': 925,\n",
       " 'featured': 926,\n",
       " 'musical': 927,\n",
       " 'status': 928,\n",
       " 'mon': 929,\n",
       " 'hot': 930,\n",
       " 'business': 931,\n",
       " 'composer': 932,\n",
       " 'le': 933,\n",
       " 'spain': 934,\n",
       " 'romania': 935,\n",
       " 'base': 936,\n",
       " 'soviet': 937,\n",
       " 'mã': 938,\n",
       " 'reached': 939,\n",
       " '1973': 940,\n",
       " '1972': 941,\n",
       " '1985': 942,\n",
       " 'sã': 943,\n",
       " 'mary': 944,\n",
       " 'academy': 945,\n",
       " 'possible': 946,\n",
       " 'brand': 947,\n",
       " 'unit': 948,\n",
       " '1967': 949,\n",
       " 'upper': 950,\n",
       " '1969': 951,\n",
       " 'becoming': 952,\n",
       " 'return': 953,\n",
       " 'feature': 954,\n",
       " 'directly': 955,\n",
       " 'characters': 956,\n",
       " 'ground': 957,\n",
       " 'my': 958,\n",
       " 'media': 959,\n",
       " 'producer': 960,\n",
       " 'b': 961,\n",
       " 'pressure': 962,\n",
       " 'bridge': 963,\n",
       " 'we': 964,\n",
       " 'multiple': 965,\n",
       " \"n't\": 966,\n",
       " 'schools': 967,\n",
       " 'variety': 968,\n",
       " 'culture': 969,\n",
       " 'eight': 970,\n",
       " 'help': 971,\n",
       " 'beach': 972,\n",
       " 'particular': 973,\n",
       " 'refer': 974,\n",
       " 'big': 975,\n",
       " 'nobel': 976,\n",
       " 'students': 977,\n",
       " 'blood': 978,\n",
       " 'acid': 979,\n",
       " 'che': 980,\n",
       " 'depression': 981,\n",
       " 'mainly': 982,\n",
       " 'news': 983,\n",
       " 'real': 984,\n",
       " 'fiction': 985,\n",
       " 'able': 986,\n",
       " 'thought': 987,\n",
       " 'certain': 988,\n",
       " 'date': 989,\n",
       " '1971': 990,\n",
       " 'remains': 991,\n",
       " 'distance': 992,\n",
       " '1988': 993,\n",
       " 'j.': 994,\n",
       " 'go': 995,\n",
       " '...': 996,\n",
       " 'museum': 997,\n",
       " 'article': 998,\n",
       " 'operating': 999,\n",
       " 'individual': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = pad_sequences(input_sequences, maxlen=max_encoder_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data = pad_sequences(output_sequences,maxlen=max_decoder_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(117952, 236)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices=np.arange(input_data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=input_data[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets=output_data[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_split=0.2\n",
    "num_validation_samples = int(val_split * data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = data[:-num_validation_samples]\n",
    "y_train = targets[:-num_validation_samples]\n",
    "x_val = data[-num_validation_samples:]\n",
    "y_val = targets[-num_validation_samples:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Trained Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.15 s, sys: 1.28 s, total: 2.43 s\n",
      "Wall time: 3.89 s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# with open('../data/glove.6B/glove.6B.50d.txt','r',encoding='utf-8') as f:\n",
    "#     lines = f.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab = {}\n",
    "# for line in lines:\n",
    "#     line = line.split(' ')\n",
    "#     key = line[0]\n",
    "#     line.remove(key)\n",
    "#     this_vec = [float(val) for val in line]\n",
    "#     vocab[key]=this_vec\n",
    "# vocab_50 = vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.34 s, sys: 20.6 s, total: 28.9 s\n",
      "Wall time: 52.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with open('../data/glove.6B/glove.6B.300d.txt','r',encoding='utf-8') as f:\n",
    "    lines = f.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "vocab = {}\n",
    "for line in lines:\n",
    "    line = line.split(' ')\n",
    "    key = line[0]\n",
    "    line.remove(key)\n",
    "    this_vec = [float(val) for val in line]\n",
    "    vocab[key]=this_vec\n",
    "vocab_300 = vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_token_index = dict([(word,i) for i,word in enumerate(input_words)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_token_index = dict([(word,i) for i,word in enumerate(target_words)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_token_index={}\n",
    "# for i,word in enumerate(input_words):\n",
    "#     try:\n",
    "#         input_token_index[i]=vocab_300[word]\n",
    "#     except KeyError:\n",
    "#         input_token_index[i]=vocab_300['unk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_token_index={}\n",
    "# for i,word in enumerate(target_words):\n",
    "#     try:\n",
    "#         target_token_index[i]=vocab_300[word]\n",
    "#     except KeyError:\n",
    "#         target_token_index[i]=vocab_300['unk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length, latent_dim),\n",
    "    dtype='float32')\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, latent_dim),\n",
    "    dtype='float32')\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, latent_dim),\n",
    "    dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(117952, 236, 300)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (input_text,target_text) in enumerate(zip(input_texts,target_texts)):\n",
    "    for j, word in enumerate(input_text):\n",
    "        try:\n",
    "            encoder_input_data[i,j] = vocab_300[word]\n",
    "        except KeyError:\n",
    "            encoder_input_data[i,j] = vocab_300['unk']\n",
    "    for j, word in enumerate(target_text):\n",
    "        try:\n",
    "            decoder_input_data[i,j] = vocab_300[word]\n",
    "        except:\n",
    "            decoder_input_data[i,j] = vocab_300['unk']\n",
    "        if j > 0:\n",
    "            try:\n",
    "                decoder_input_data[i,j-1]=vocab_300[word]\n",
    "            except:\n",
    "                decoder_input_data[i,j-1]=vocab_300['unk']            \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(117952, 236, 300)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input 0 is incompatible with layer gru_7: expected ndim=3, found ndim=2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-86-4653cc3e7954>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGRU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_encoder_seq_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRepeatVector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_decoder_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGRU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/keras/engine/sequential.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    163\u001b[0m                     \u001b[0;31m# and create the node connecting the current layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m                     \u001b[0;31m# to the input layer we just created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m                     \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m                     \u001b[0mset_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/keras/layers/recurrent.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconstants\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m         \u001b[0;31m# If any of `initial_state` or `constants` are specified and are Keras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    412\u001b[0m                 \u001b[0;31m# Raise exceptions in case the input is not compatible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m                 \u001b[0;31m# with the input_spec specified in the layer constructor.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_input_compatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m                 \u001b[0;31m# Collect input shapes to build layer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                                      \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m': expected ndim='\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                                      \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m', found ndim='\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m                                      str(K.ndim(x)))\n\u001b[0m\u001b[1;32m    312\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_ndim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m                 \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input 0 is incompatible with layer gru_7: expected ndim=3, found ndim=2"
     ]
    }
   ],
   "source": [
    "learning_rate=1e-3\n",
    "model = Sequential()\n",
    "model.add(GRU(128,input_shape=(max_encoder_seq_length,),return_sequences=False))\n",
    "model.add(RepeatVector(max_decoder_length))\n",
    "model.add(GRU(128,return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(len(target_words),activation='softmax')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "x = Embedding(num_encoder_tokens, latent_dim)(encoder_inputs)\n",
    "x, state_h, state_c = LSTM(latent_dim,\n",
    "                           return_state=True)(x)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "x = Embedding(num_decoder_tokens, latent_dim)(decoder_inputs)\n",
    "x = LSTM(latent_dim, return_sequences=True)(x, initial_state=encoder_states)\n",
    "decoder_outputs = Dense(num_decoder_tokens, activation='softmax')(x)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Compile & run training\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "# Note that `decoder_target_data` needs to be one-hot encoded,\n",
    "# rather than sequences of integers like `decoder_input_data`!\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specific Trained Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constrained Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ppdb = pd.read_csv('../data/ppdb-2.0-s-all',sep='|||')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "xin = Input(batch_shape=(batch_size, seq_size), dtype='int32')\n",
    "xemb = Embedding(embedding_size, mask_zero=True)(xin)\n",
    "\n",
    "rnn_fwd1 = LSTM(rnn_size, return_sequence=True)(xemb)\n",
    "rnn_bwd1 = LSTM(rnn_size, return_sequence=True, go_backwards=True)(xemb)\n",
    "rnn_bidir1 = merge([rnn_fwd1, rnn_bwd1], mode='concat')\n",
    "\n",
    "predictions = TimeDistributed(Dense(output_class_size, activation='softmax'))(rnn_bidir1) \n",
    "\n",
    "model = Model(input=xin, output=predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
